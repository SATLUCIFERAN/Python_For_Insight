{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d0a17b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8613910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_order_data = {\n",
    "    'OrderID': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
    "    'CustomerID': ['C001', 'C002', 'C003', 'C001', 'C004',\n",
    "                   'C005', 'C006', 'C007', 'C008', 'C009'],\n",
    "    'OrderDate': ['2023-01-15', '2023-02-20', '2023-03-10', '2023-01-10', '2023-04-05',\n",
    "                  '2023-02-28', '2023-07-01', '2023-06-15', '2023-05-20', '2023-08-01'],\n",
    "    'ProductCategory': ['Electronics', 'Books', 'Food', 'Electronics', 'Apparel',\n",
    "                        'Books', 'Electronics', 'Food', 'InvalidCategory', 'Apparel'], \n",
    "    'Quantity': [1, 2, 0, 3, 1, 5, -2, 1, 4, 2], \n",
    "    'UnitPrice': [150.00, 25.50, 10.00, 150.00, 50.00, 25.50, 200.00, 12.00, 30.00, 45.00],\n",
    "    'TotalPrice': [150.00, 51.00, 0.00, 450.00, 50.00, 127.50, -400.00, 12.00, 120.00, 45.00], \n",
    "    'PaymentStatus': ['Paid', 'Pending', 'Paid', 'Paid', 'Pending',\n",
    "                      'Completed', 'Paid', 'Pending', 'Paid', 'Refunded'], \n",
    "    'DeliveryDate': ['2023-01-20', '2023-02-25', '2023-03-12', '2023-01-12', '2023-04-10',\n",
    "                     '2023-02-27', '2023-06-25', '2023-06-10', '2023-05-25', '2023-07-25'] \n",
    "}\n",
    "\n",
    "df_orders = pd.DataFrame(raw_order_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The Initial Case File ---\n",
      "|   OrderID | CustomerID   | OrderDate           | ProductCategory   |   Quantity |   UnitPrice |   TotalPrice | PaymentStatus   | DeliveryDate        |\n",
      "|----------:|:-------------|:--------------------|:------------------|-----------:|------------:|-------------:|:----------------|:--------------------|\n",
      "|       101 | C001         | 2023-01-15 00:00:00 | Electronics       |          1 |       150   |        150   | Paid            | 2023-01-20 00:00:00 |\n",
      "|       102 | C002         | 2023-02-20 00:00:00 | Books             |          2 |        25.5 |         51   | Pending         | 2023-02-25 00:00:00 |\n",
      "|       103 | C003         | 2023-03-10 00:00:00 | Food              |          0 |        10   |          0   | Paid            | 2023-03-12 00:00:00 |\n",
      "|       104 | C001         | 2023-01-10 00:00:00 | Electronics       |          3 |       150   |        450   | Paid            | 2023-01-12 00:00:00 |\n",
      "|       105 | C004         | 2023-04-05 00:00:00 | Apparel           |          1 |        50   |         50   | Pending         | 2023-04-10 00:00:00 |\n",
      "|       106 | C005         | 2023-02-28 00:00:00 | Books             |          5 |        25.5 |        127.5 | Completed       | 2023-02-27 00:00:00 |\n",
      "|       107 | C006         | 2023-07-01 00:00:00 | Electronics       |         -2 |       200   |       -400   | Paid            | 2023-06-25 00:00:00 |\n",
      "|       108 | C007         | 2023-06-15 00:00:00 | Food              |          1 |        12   |         12   | Pending         | 2023-06-10 00:00:00 |\n",
      "|       109 | C008         | 2023-05-20 00:00:00 | InvalidCategory   |          4 |        30   |        120   | Paid            | 2023-05-25 00:00:00 |\n",
      "|       110 | C009         | 2023-08-01 00:00:00 | Apparel           |          2 |        45   |         45   | Refunded        | 2023-07-25 00:00:00 |\n"
     ]
    }
   ],
   "source": [
    "df_orders['OrderDate'] = pd.to_datetime(df_orders['OrderDate'])\n",
    "df_orders['DeliveryDate'] = pd.to_datetime(df_orders['DeliveryDate'])\n",
    "\n",
    "df_clean = df_orders.copy()\n",
    "\n",
    "print(\"--- The Initial Case File ---\")\n",
    "print(df_clean.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fb07824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Lies Found: Non-Positive Quantities ---\n",
      "|   OrderID | CustomerID   | OrderDate           | ProductCategory   |   Quantity |   UnitPrice |   TotalPrice | PaymentStatus   | DeliveryDate        |\n",
      "|----------:|:-------------|:--------------------|:------------------|-----------:|------------:|-------------:|:----------------|:--------------------|\n",
      "|       103 | C003         | 2023-03-10 00:00:00 | Food              |          0 |          10 |            0 | Paid            | 2023-03-12 00:00:00 |\n",
      "|       107 | C006         | 2023-07-01 00:00:00 | Electronics       |         -2 |         200 |         -400 | Paid            | 2023-06-25 00:00:00 |\n"
     ]
    }
   ],
   "source": [
    "non_positive_quantity_mask = df_clean['Quantity'] <= 0\n",
    "print(\"\\n--- Lies Found: Non-Positive Quantities ---\")\n",
    "print(df_clean[non_positive_quantity_mask].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a4e688a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Lies Found: Non-Positive Total Prices ---\n",
      "|   OrderID | CustomerID   | OrderDate           | ProductCategory   |   Quantity |   UnitPrice |   TotalPrice | PaymentStatus   | DeliveryDate        |\n",
      "|----------:|:-------------|:--------------------|:------------------|-----------:|------------:|-------------:|:----------------|:--------------------|\n",
      "|       103 | C003         | 2023-03-10 00:00:00 | Food              |          0 |          10 |            0 | Paid            | 2023-03-12 00:00:00 |\n",
      "|       107 | C006         | 2023-07-01 00:00:00 | Electronics       |         -2 |         200 |         -400 | Paid            | 2023-06-25 00:00:00 |\n"
     ]
    }
   ],
   "source": [
    "non_positive_total_price_mask = df_clean['TotalPrice'] <= 0\n",
    "print(\"\\n--- Lies Found: Non-Positive Total Prices ---\")\n",
    "print(df_clean[non_positive_total_price_mask].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1a097e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Our Data After the Boundary Sentinel's Corrections ---\n",
      "|   OrderID | CustomerID   | OrderDate           | ProductCategory   |   Quantity |   UnitPrice |   TotalPrice | PaymentStatus   | DeliveryDate        |\n",
      "|----------:|:-------------|:--------------------|:------------------|-----------:|------------:|-------------:|:----------------|:--------------------|\n",
      "|       101 | C001         | 2023-01-15 00:00:00 | Electronics       |          1 |       150   |        150   | Paid            | 2023-01-20 00:00:00 |\n",
      "|       102 | C002         | 2023-02-20 00:00:00 | Books             |          2 |        25.5 |         51   | Pending         | 2023-02-25 00:00:00 |\n",
      "|       103 | C003         | 2023-03-10 00:00:00 | Food              |        nan |        10   |        nan   | Paid            | 2023-03-12 00:00:00 |\n",
      "|       104 | C001         | 2023-01-10 00:00:00 | Electronics       |          3 |       150   |        450   | Paid            | 2023-01-12 00:00:00 |\n",
      "|       105 | C004         | 2023-04-05 00:00:00 | Apparel           |          1 |        50   |         50   | Pending         | 2023-04-10 00:00:00 |\n",
      "|       106 | C005         | 2023-02-28 00:00:00 | Books             |          5 |        25.5 |        127.5 | Completed       | 2023-02-27 00:00:00 |\n",
      "|       107 | C006         | 2023-07-01 00:00:00 | Electronics       |        nan |       200   |        nan   | Paid            | 2023-06-25 00:00:00 |\n",
      "|       108 | C007         | 2023-06-15 00:00:00 | Food              |          1 |        12   |         12   | Pending         | 2023-06-10 00:00:00 |\n",
      "|       109 | C008         | 2023-05-20 00:00:00 | InvalidCategory   |          4 |        30   |        120   | Paid            | 2023-05-25 00:00:00 |\n",
      "|       110 | C009         | 2023-08-01 00:00:00 | Apparel           |          2 |        45   |         45   | Refunded        | 2023-07-25 00:00:00 |\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_orders.copy() \n",
    "df_clean.loc[df_clean['Quantity'] <= 0, 'Quantity'] = np.nan\n",
    "df_clean.loc[df_clean['TotalPrice'] <= 0, 'TotalPrice'] = np.nan\n",
    "print(\"\\n--- Our Data After the Boundary Sentinel's Corrections ---\")\n",
    "print(df_clean.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be5ff15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Our Official Product Categories Dictionary ---\n",
      "['Electronics', 'Books', 'Food', 'Apparel']\n",
      "\n",
      "--- Our Official Payment Status Dictionary ---\n",
      "['Paid', 'Pending', 'Refunded']\n"
     ]
    }
   ],
   "source": [
    "valid_product_categories = ['Electronics', 'Books', 'Food', 'Apparel']\n",
    "valid_payment_statuses = ['Paid', 'Pending', 'Refunded']\n",
    "\n",
    "print(\"\\n--- Our Official Product Categories Dictionary ---\")\n",
    "print(valid_product_categories)\n",
    "print(\"\\n--- Our Official Payment Status Dictionary ---\")\n",
    "print(valid_payment_statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302cd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ProductCategory entries not in our official dictionary ---\n",
      "|   OrderID | CustomerID   | OrderDate           | ProductCategory   |   Quantity |   UnitPrice |   TotalPrice | PaymentStatus   | DeliveryDate        |\n",
      "|----------:|:-------------|:--------------------|:------------------|-----------:|------------:|-------------:|:----------------|:--------------------|\n",
      "|       109 | C008         | 2023-05-20 00:00:00 | InvalidCategory   |          4 |          30 |          120 | Paid            | 2023-05-25 00:00:00 |\n"
     ]
    }
   ],
   "source": [
    "invalid_category_mask = ~df_clean['ProductCategory'].\\\n",
    "                         isin(valid_product_categories)\n",
    "\n",
    "print(\"\\n--- ProductCategory entries not in our official dictionary ---\")\n",
    "print(df_clean[invalid_category_mask].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2d4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PaymentStatus entries not in our official dictionary ---\n",
      "|   OrderID | CustomerID   | OrderDate           | ProductCategory   |   Quantity |   UnitPrice |   TotalPrice | PaymentStatus   | DeliveryDate        |\n",
      "|----------:|:-------------|:--------------------|:------------------|-----------:|------------:|-------------:|:----------------|:--------------------|\n",
      "|       106 | C005         | 2023-02-28 00:00:00 | Books             |          5 |        25.5 |        127.5 | Completed       | 2023-02-27 00:00:00 |\n"
     ]
    }
   ],
   "source": [
    "invalid_status_mask = ~df_clean['PaymentStatus'].\\\n",
    "                       isin(valid_payment_statuses)\n",
    "\n",
    "print(\"\\n--- PaymentStatus entries not in our official dictionary ---\")\n",
    "print(df_clean[invalid_status_mask].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c228f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.loc[df_clean['ProductCategory'] == \n",
    "            'InvalidCategory', 'ProductCategory'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04399909",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['PaymentStatus'] = df_clean['PaymentStatus'].\\\n",
    "                            replace('Completed', 'Paid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1621837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Our Data After the Interpreter's Corrections ---\n",
      "|   OrderID | CustomerID   | OrderDate           | ProductCategory   |   Quantity |   UnitPrice |   TotalPrice | PaymentStatus   | DeliveryDate        |\n",
      "|----------:|:-------------|:--------------------|:------------------|-----------:|------------:|-------------:|:----------------|:--------------------|\n",
      "|       101 | C001         | 2023-01-15 00:00:00 | Electronics       |          1 |       150   |        150   | Paid            | 2023-01-20 00:00:00 |\n",
      "|       102 | C002         | 2023-02-20 00:00:00 | Books             |          2 |        25.5 |         51   | Pending         | 2023-02-25 00:00:00 |\n",
      "|       103 | C003         | 2023-03-10 00:00:00 | Food              |        nan |        10   |        nan   | Paid            | 2023-03-12 00:00:00 |\n",
      "|       104 | C001         | 2023-01-10 00:00:00 | Electronics       |          3 |       150   |        450   | Paid            | 2023-01-12 00:00:00 |\n",
      "|       105 | C004         | 2023-04-05 00:00:00 | Apparel           |          1 |        50   |         50   | Pending         | 2023-04-10 00:00:00 |\n",
      "|       106 | C005         | 2023-02-28 00:00:00 | Books             |          5 |        25.5 |        127.5 | Paid            | 2023-02-27 00:00:00 |\n",
      "|       107 | C006         | 2023-07-01 00:00:00 | Electronics       |        nan |       200   |        nan   | Paid            | 2023-06-25 00:00:00 |\n",
      "|       108 | C007         | 2023-06-15 00:00:00 | Food              |          1 |        12   |         12   | Pending         | 2023-06-10 00:00:00 |\n",
      "|       109 | C008         | 2023-05-20 00:00:00 | nan               |          4 |        30   |        120   | Paid            | 2023-05-25 00:00:00 |\n",
      "|       110 | C009         | 2023-08-01 00:00:00 | Apparel           |          2 |        45   |         45   | Refunded        | 2023-07-25 00:00:00 |\n",
      "\n",
      "--- Final Payment Status Counts (Unified) ---\n",
      "| PaymentStatus   |   count |\n",
      "|:----------------|--------:|\n",
      "| Paid            |       6 |\n",
      "| Pending         |       3 |\n",
      "| Refunded        |       1 |\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Our Data After the Interpreter's Corrections ---\")\n",
    "print(df_clean.to_markdown(index=False))\n",
    "print(\"\\n--- Final Payment Status Counts (Unified) ---\")\n",
    "print(df_clean['PaymentStatus'].value_counts().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1672a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- The Master Logician's Report (True = Consistent, False = Inconsistent) ---\n",
      "[ True  True  True  True  True  True  True  True  True False]\n",
      "\n",
      "--- The False Alibis: Rows with Inconsistent Totals ---\n",
      "|   OrderID | CustomerID   | OrderDate           | ProductCategory   |   Quantity |   UnitPrice |   TotalPrice | PaymentStatus   | DeliveryDate        |\n",
      "|----------:|:-------------|:--------------------|:------------------|-----------:|------------:|-------------:|:----------------|:--------------------|\n",
      "|       110 | C009         | 2023-08-01 00:00:00 | Apparel           |          2 |          45 |           45 | Refunded        | 2023-07-25 00:00:00 |\n"
     ]
    }
   ],
   "source": [
    "calculated_total_price = df_clean['Quantity'] * df_clean['UnitPrice']\n",
    "is_consistent_total_price = np.isclose(df_clean['TotalPrice'],\n",
    "                                       calculated_total_price, \n",
    "                                       equal_nan=True)\n",
    "\n",
    "print(\"\\n--- The Master Logician's Report \"\n",
    "      \"(True = Consistent, False = Inconsistent) ---\")\n",
    "print(is_consistent_total_price)\n",
    "\n",
    "inconsistent_total_price_rows = df_clean[~is_consistent_total_price]\n",
    "print(\"\\n--- The False Alibis: Rows with Inconsistent Totals ---\")\n",
    "print(inconsistent_total_price_rows.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e4fadbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_consistent_total_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de2c450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Our Data After the Master Logician's Corrections (after 3.6.3) ---\n",
      "|   OrderID | CustomerID   | OrderDate           | ProductCategory   |   Quantity |   UnitPrice |   TotalPrice | PaymentStatus   | DeliveryDate        |\n",
      "|----------:|:-------------|:--------------------|:------------------|-----------:|------------:|-------------:|:----------------|:--------------------|\n",
      "|       101 | C001         | 2023-01-15 00:00:00 | Electronics       |          1 |       150   |        150   | Paid            | 2023-01-20 00:00:00 |\n",
      "|       102 | C002         | 2023-02-20 00:00:00 | Books             |          2 |        25.5 |         51   | Pending         | 2023-02-25 00:00:00 |\n",
      "|       103 | C003         | 2023-03-10 00:00:00 | Food              |        nan |        10   |        nan   | Paid            | 2023-03-12 00:00:00 |\n",
      "|       104 | C001         | 2023-01-10 00:00:00 | Electronics       |          3 |       150   |        450   | Paid            | 2023-01-12 00:00:00 |\n",
      "|       105 | C004         | 2023-04-05 00:00:00 | Apparel           |          1 |        50   |         50   | Pending         | 2023-04-10 00:00:00 |\n",
      "|       106 | C005         | 2023-02-28 00:00:00 | Books             |          5 |        25.5 |        127.5 | Paid            | 2023-02-27 00:00:00 |\n",
      "|       107 | C006         | 2023-07-01 00:00:00 | Electronics       |        nan |       200   |        nan   | Paid            | 2023-06-25 00:00:00 |\n",
      "|       108 | C007         | 2023-06-15 00:00:00 | Food              |          1 |        12   |         12   | Pending         | 2023-06-10 00:00:00 |\n",
      "|       109 | C008         | 2023-05-20 00:00:00 | nan               |          4 |        30   |        120   | Paid            | 2023-05-25 00:00:00 |\n",
      "|       110 | C009         | 2023-08-01 00:00:00 | Apparel           |          2 |        45   |         90   | Refunded        | 2023-07-25 00:00:00 |\n"
     ]
    }
   ],
   "source": [
    "df_clean.loc[df_clean['OrderID'] == 110, 'TotalPrice'] = \\\n",
    "df_clean.loc[df_clean['OrderID'] == 110, 'Quantity'] * \\\n",
    "df_clean.loc[df_clean['OrderID'] == 110, 'UnitPrice']\n",
    "\n",
    "print(\"\\n--- Our Data After the Master Logician's Corrections ---\")\n",
    "print(df_clean.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4e73ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- The Timekeeper's Report: Rows with Paradoxical Dates ---\n",
      "|   OrderID | CustomerID   | OrderDate           | ProductCategory   |   Quantity |   UnitPrice |   TotalPrice | PaymentStatus   | DeliveryDate        |\n",
      "|----------:|:-------------|:--------------------|:------------------|-----------:|------------:|-------------:|:----------------|:--------------------|\n",
      "|       106 | C005         | 2023-02-28 00:00:00 | Books             |          5 |        25.5 |        127.5 | Paid            | 2023-02-27 00:00:00 |\n",
      "|       107 | C006         | 2023-07-01 00:00:00 | Electronics       |        nan |       200   |        nan   | Paid            | 2023-06-25 00:00:00 |\n",
      "|       108 | C007         | 2023-06-15 00:00:00 | Food              |          1 |        12   |         12   | Pending         | 2023-06-10 00:00:00 |\n",
      "|       110 | C009         | 2023-08-01 00:00:00 | Apparel           |          2 |        45   |         90   | Refunded        | 2023-07-25 00:00:00 |\n"
     ]
    }
   ],
   "source": [
    "time_paradox_mask = df_clean['DeliveryDate'] < df_clean['OrderDate']\n",
    "\n",
    "print(\"\\n--- The Timekeeper's Report: Rows with Paradoxical Dates ---\")\n",
    "print(df_clean[time_paradox_mask].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb77611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Our Data After the Timekeeper's Corrections (after 3.6.4) ---\n",
      "|   OrderID | CustomerID   | OrderDate           | ProductCategory   |   Quantity |   UnitPrice |   TotalPrice | PaymentStatus   | DeliveryDate        |\n",
      "|----------:|:-------------|:--------------------|:------------------|-----------:|------------:|-------------:|:----------------|:--------------------|\n",
      "|       101 | C001         | 2023-01-15 00:00:00 | Electronics       |          1 |       150   |        150   | Paid            | 2023-01-20 00:00:00 |\n",
      "|       102 | C002         | 2023-02-20 00:00:00 | Books             |          2 |        25.5 |         51   | Pending         | 2023-02-25 00:00:00 |\n",
      "|       103 | C003         | 2023-03-10 00:00:00 | Food              |        nan |        10   |        nan   | Paid            | 2023-03-12 00:00:00 |\n",
      "|       104 | C001         | 2023-01-10 00:00:00 | Electronics       |          3 |       150   |        450   | Paid            | 2023-01-12 00:00:00 |\n",
      "|       105 | C004         | 2023-04-05 00:00:00 | Apparel           |          1 |        50   |         50   | Pending         | 2023-04-10 00:00:00 |\n",
      "|       106 | C005         | 2023-02-28 00:00:00 | Books             |          5 |        25.5 |        127.5 | Paid            | NaT                 |\n",
      "|       107 | C006         | 2023-07-01 00:00:00 | Electronics       |        nan |       200   |        nan   | Paid            | NaT                 |\n",
      "|       108 | C007         | 2023-06-15 00:00:00 | Food              |          1 |        12   |         12   | Pending         | NaT                 |\n",
      "|       109 | C008         | 2023-05-20 00:00:00 | nan               |          4 |        30   |        120   | Paid            | 2023-05-25 00:00:00 |\n",
      "|       110 | C009         | 2023-08-01 00:00:00 | Apparel           |          2 |        45   |         90   | Refunded        | NaT                 |\n"
     ]
    }
   ],
   "source": [
    "df_clean.loc[time_paradox_mask, 'DeliveryDate'] = np.nan\n",
    "print(\"\\n--- Our Data After the Timekeeper's Corrections ---\")\n",
    "print(df_clean.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3e6db1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- The Unverified Email List ---\n",
      "| CustomerID   | EmailAddress         |\n",
      "|:-------------|:---------------------|\n",
      "| C1001        | user1@email.com      |\n",
      "| C1002        | user2@domain         |\n",
      "| C1003        | invalid-email.com    |\n",
      "| C1004        | user_three@web.org   |\n",
      "| C1005        | user4@email@corp.net |\n",
      "\n",
      "--- The Validator's Report (True = Valid, False = Invalid) ---\n",
      "|    |   EmailAddress |\n",
      "|---:|---------------:|\n",
      "|  0 |              1 |\n",
      "|  1 |              0 |\n",
      "|  2 |              0 |\n",
      "|  3 |              1 |\n",
      "|  4 |              1 |\n",
      "\n",
      "--- The Malformed Entries ---\n",
      "| CustomerID   | EmailAddress      |\n",
      "|:-------------|:------------------|\n",
      "| C1002        | user2@domain      |\n",
      "| C1003        | invalid-email.com |\n"
     ]
    }
   ],
   "source": [
    "email_data = {\n",
    "    'CustomerID': ['C1001', 'C1002', 'C1003', 'C1004', 'C1005'],\n",
    "    'EmailAddress': ['user1@email.com', 'user2@domain',\n",
    "                     'invalid-email.com', 'user_three@web.org', \n",
    "                     'user4@email@corp.net']\n",
    "}\n",
    "df_emails = pd.DataFrame(email_data)\n",
    "\n",
    "print(\"\\n--- The Unverified Email List ---\")\n",
    "print(df_emails.to_markdown(index=False))\n",
    "\n",
    "email_pattern = r'^\\S+@\\S+\\.\\S+$'\n",
    "is_valid_email = df_emails['EmailAddress'].str.match(email_pattern)\n",
    "\n",
    "print(\"\\n--- The Validator's Report (True = Valid, False = Invalid) ---\")\n",
    "print(is_valid_email.to_markdown())\n",
    "\n",
    "invalid_emails_found = df_emails[~is_valid_email]\n",
    "print(\"\\n--- The Malformed Entries ---\")\n",
    "print(invalid_emails_found.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01e43b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- The Validator's Report (True = Valid, False = Invalid) ---\n",
      "|    |   EmailAddress |\n",
      "|---:|---------------:|\n",
      "|  0 |              1 |\n",
      "|  1 |              0 |\n",
      "|  2 |              0 |\n",
      "|  3 |              1 |\n",
      "|  4 |              1 |\n"
     ]
    }
   ],
   "source": [
    "email_pattern = r'^\\S+@\\S+\\.\\S+$'\n",
    "is_valid_email = df_emails['EmailAddress'].str.match(email_pattern)\n",
    "\n",
    "print(\"\\n--- The Validator's Report (True = Valid, False = Invalid) ---\")\n",
    "print(is_valid_email.to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4379bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- The Malformed Entries ---\n",
      "| CustomerID   | EmailAddress      |\n",
      "|:-------------|:------------------|\n",
      "| C1002        | user2@domain      |\n",
      "| C1003        | invalid-email.com |\n"
     ]
    }
   ],
   "source": [
    "invalid_emails_found = df_emails[~is_valid_email]\n",
    "print(\"\\n--- The Malformed Entries ---\")\n",
    "print(invalid_emails_found.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d7dd7429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Our Data After the Format Enforcer's Corrections (after 3.6.5) ---\n",
      "| CustomerID   | EmailAddress         |\n",
      "|:-------------|:---------------------|\n",
      "| C1001        | user1@email.com      |\n",
      "| C1002        | nan                  |\n",
      "| C1003        | nan                  |\n",
      "| C1004        | user_three@web.org   |\n",
      "| C1005        | user4@email@corp.net |\n"
     ]
    }
   ],
   "source": [
    "df_emails_clean = df_emails.copy()\n",
    "invalid_email_mask_to_apply = ~df_emails_clean['EmailAddress'].str.match(email_pattern)\n",
    "df_emails_clean.loc[invalid_email_mask_to_apply, 'EmailAddress'] = np.nan\n",
    "print(\"\\n--- Our Data After the Format Enforcer's Corrections (after 3.6.5) ---\")\n",
    "print(df_emails_clean.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0663f377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Our Data After the Format Enforcer's Corrections ---\n",
      "| CustomerID   | EmailAddress         |\n",
      "|:-------------|:---------------------|\n",
      "| C1001        | user1@email.com      |\n",
      "| C1002        | nan                  |\n",
      "| C1003        | nan                  |\n",
      "| C1004        | user_three@web.org   |\n",
      "| C1005        | user4@email@corp.net |\n"
     ]
    }
   ],
   "source": [
    "df_emails_clean = df_emails.copy()\n",
    "\n",
    "invalid_email_mask_to_apply = \\\n",
    "~df_emails_clean['EmailAddress'].str.match(email_pattern)\n",
    "\n",
    "df_emails_clean.loc[invalid_email_mask_to_apply, 'EmailAddress'] = np.nan\n",
    "print(\"\\n--- Our Data After the Format Enforcer's Corrections ---\")\n",
    "print(df_emails_clean.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44cfe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
