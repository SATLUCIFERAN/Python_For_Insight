{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d2eb114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. The Mock Artifacts: Creating a more complex dataset with mixed types ---\n",
    "# We'll create a dataset that has both numerical and categorical features,\n",
    "# similar to what you would see in the real world.\n",
    "\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'duration_minutes': np.random.uniform(90, 180, size=100),\n",
    "    'budget_millions': np.random.uniform(5, 300, size=100),\n",
    "    'genre': np.random.choice(['Sci-Fi', 'Action', 'Comedy', 'Drama'], \n",
    "                              size=100),    \n",
    "}\n",
    "movies_df = pd.DataFrame(data)\n",
    "\n",
    "# Setting 'is_hit'\n",
    "# A movie is now a \"hit\" if its budget is above the median budget\n",
    "movies_df['is_hit'] = (movies_df['budget_millions'] > movies_df['budget_millions'].median()).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "604a036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Preparing the Ritual: Defining our preprocessor and model ---\n",
    "# First, we identify our numerical and categorical features.\n",
    "\n",
    "numerical_features = ['duration_minutes', 'budget_millions']\n",
    "categorical_features = ['genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5aaeabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the preprocessor using a ColumnTransformer.\n",
    "# This will apply StandardScaler to numeric data and OneHotEncoder to categorical data.\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60a8dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define our model.\n",
    "model = DecisionTreeClassifier(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17e0bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. The Grand Ritual: Building and training the Pipeline ---\n",
    "# The pipeline links the preprocessor and the model together.\n",
    "# This ensures every step is applied in the correct order.\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', model)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "565f1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate our features (X) and our target (y)\n",
    "X = movies_df.drop('is_hit', axis=1)\n",
    "y = movies_df['is_hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78d09fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38c0fd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the pipeline...\n",
      "Pipeline training complete.\n"
     ]
    }
   ],
   "source": [
    "# Now, we train the entire pipeline on the training data.\n",
    "# The .fit() command automatically applies the preprocessor and then trains the model.\n",
    "\n",
    "print(\"Training the pipeline...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d925c510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing 5-Fold Cross-Validation on the entire pipeline ---\n",
      "Cross-Validation scores (5 folds): [1.   1.   0.95 1.   1.  ]\n",
      "Average Cross-Validation score: 0.9900\n"
     ]
    }
   ],
   "source": [
    "# --- 4. The Grand Revelation: Evaluating the pipeline's prophecy ---\n",
    "\n",
    "print(\"\\n--- Performing 5-Fold Cross-Validation on the entire pipeline ---\")\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5)\n",
    "\n",
    "print(f\"Cross-Validation scores (5 folds): {cv_scores}\")\n",
    "print(f\"Average Cross-Validation score: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545886c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
